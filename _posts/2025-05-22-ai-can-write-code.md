---
layout: post
title: "AI Can Write Code, But It Can’t Understand It"
date: 2025-05-22
excerpt: "AI tools like Copilot and ChatGPT can generate code — but they don’t understand it. True engineering still requires human judgment, structure, and care."
read_time: true
author_profile: true
tags:
  - ai
  - software-engineering
  - sustainability
  - cognition
--- 

Let’s get one thing straight: AI can write code.

Sometimes it’s brilliant. Sometimes it’s uncanny. Tools like Cursor, Copilot,  
and ChatGPT have changed how I work — and I’m not going back.

But we need to talk about what AI **can’t** do.

Because writing code is easy. Understanding it is hard.

And right now, we’re at risk of building faster than we can comprehend.

<!--more-->

---

## Code is not just syntax

AI can generate a Rails model or a React hook or a Terraform module.  
It can refactor a method. It can fill in your tests. That’s not the hard part.

The hard part is understanding **why the code exists**, **how it behaves under  
stress**, **what it breaks when it changes**, and **what it means to the people  
who use or maintain it**.

That’s not pattern-matching. That’s **thinking**.

And no LLM is doing that — not yet.

---

## Understanding is multi-layered

AI sees the tokens. It doesn’t see the context.

You and I look at a piece of code and think:

- What’s the business rule here?  
- What assumption is this making?  
- Who’s going to be woken up if this breaks at 2am?  
- Is this the right place for this logic to live?  
- How does this shape our ability to evolve?

An LLM is doing autocomplete. You’re doing analysis.  
Those are different games.

---

## AI scales productivity. Humans scale sustainability.

AI lets us write more code faster.

But more code isn’t always good. It can mean more bugs, more interfaces, more  
complexity, more entropy — unless someone is actively shaping the direction  
of the system.

That someone has to be you.

We are the ones who understand trade-offs. Who teach teams how things work.  
Who refactor when the design is wrong. Who build mental models.

AI doesn’t do that. It’s not a partner in design — it’s a mirror of the  
existing patterns.

And it can’t tell when those patterns are broken.

---

## You don’t scale understanding with tokens

You scale it with documentation, tests, naming, constraints, pair programming,  
reviews, and stories.

You scale it with **culture**.

If we want to use AI sustainably, we have to **build the systems it can’t see**:  
boundaries, feedback loops, conversations. Because that’s where the real  
understanding happens — not in a GPT prompt, but in the space between humans  
trying to do the right thing.

---

## Final thought

I’m not scared of AI writing code.

I’m scared of teams thinking **that’s all there is to engineering**.

Because good software isn’t just functional — it’s intentional. It has  
structure. It has stories. It has signals of care.

And we can’t prompt our way to that.
